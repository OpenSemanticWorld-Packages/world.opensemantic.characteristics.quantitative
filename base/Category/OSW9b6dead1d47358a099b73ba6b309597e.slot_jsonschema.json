{
    "@context": [
        "/wiki/Category:OSWbcaa33bd770e53e09d5e6087d141648b?action=raw&slot=jsonschema",
        {}
    ],
    "$defs": {
        "generated": {
            "$comment": "Autogenerated section - do not edit. Generated from Category:Category Category:OSWffe74f291d354037b318c422591c5023 Category:OSWac07a46c2cf14f3daec503136861f5ab",
            "allOf": [
                {
                    "$ref": "/wiki/Category:OSWbcaa33bd770e53e09d5e6087d141648b?action=raw&slot=jsonschema"
                }
            ],
            "type": "object",
            "uuid": "9b6dead1-d473-58a0-99b7-3ba6b309597e",
            "title": "GeneratedInformationEntropy",
            "title*": {
                "en": "Information Entropy"
            },
            "description": "This is an autogenerated partial class definition of 'InformationEntropy'",
            "description*": {
                "en": "Information Entropy is a concept from information theory. It tells how much information there is in an event. In general, the more uncertain or random the event is, the more information it will contain. The concept of information entropy was created by a mathematician. He was named Claude Elwood Shannon. It has applications in many areas, including lossless data compression, statistical inference, cryptography and recently in other disciplines as biology, physics or machine learning."
            },
            "required": [
                "type"
            ],
            "properties": {
                "type": {
                    "default": [
                        "Category:OSW9b6dead1d47358a099b73ba6b309597e"
                    ]
                }
            },
            "defaultProperties": [
                "type"
            ],
            "x-smw-quantity-property": "Property:HasInformationEntropyValue"
        }
    },
    "allOf": [
        {
            "$ref": "#/$defs/generated"
        }
    ],
    "title": "InformationEntropy",
    "description": "Information Entropy is a concept from information theory. It tells how much information there is in an event. In general, the more uncertain or random the event is, the more information it will contain. The concept of information entropy was created by a mathematician. He was named Claude Elwood Shannon. It has applications in many areas, including lossless data compression, statistical inference, cryptography and recently in other disciplines as biology, physics or machine learning."
}